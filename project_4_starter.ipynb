{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Multi-factor Model\n",
    "## Instructions\n",
    "Each problem consists of a function to implement and instructions on how to implement the function.  The parts of the function that need to be implemented are marked with a `# TODO` comment. After implementing the function, run the cell to test it against the unit tests we've provided. For each problem, we provide one or more unit tests from our `project_tests` package. These unit tests won't tell you if your answer is correct, but will warn you of any major errors. Your code will be checked for the correct solution when you submit it to Udacity.\n",
    "\n",
    "## Packages\n",
    "When you implement the functions, you'll only need to you use the packages you've used in the classroom, like [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/). These packages will be imported for you. We recommend you don't add any import statements, otherwise the grader might not be able to run your code.\n",
    "\n",
    "The other packages that we're importing are `project_helper` and `project_tests`. These are custom packages built to help you solve the problems.  The `project_helper` module contains utility functions and graph functions. The `project_tests` contains the unit tests for all the problems.\n",
    "\n",
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting alphalens==0.3.2 (from -r requirements.txt (line 1))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/dc/2f9cd107d0d4cf6223d37d81ddfbbdbf0d703d03669b83810fa6b97f32e5/alphalens-0.3.2.tar.gz (18.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 18.9MB 897kB/s ta 0:00:011 1% |▌                               | 296kB 9.4MB/s eta 0:00:02    4% |█▎                              | 757kB 10.1MB/s eta 0:00:02    6% |██                              | 1.2MB 7.7MB/s eta 0:00:03    14% |████▋                           | 2.7MB 16.2MB/s eta 0:00:01    16% |█████▎                          | 3.1MB 10.9MB/s eta 0:00:02    19% |██████▏                         | 3.6MB 9.9MB/s eta 0:00:02    21% |███████                         | 4.1MB 11.5MB/s eta 0:00:02    31% |██████████                      | 6.0MB 9.4MB/s eta 0:00:02    38% |████████████▎                   | 7.3MB 8.6MB/s eta 0:00:02    40% |█████████████                   | 7.7MB 10.6MB/s eta 0:00:02    45% |██████████████▋                 | 8.6MB 9.6MB/s eta 0:00:02    48% |███████████████▍                | 9.1MB 9.2MB/s eta 0:00:02    53% |█████████████████               | 10.1MB 10.2MB/s eta 0:00:01    61% |███████████████████▋            | 11.6MB 9.3MB/s eta 0:00:01    66% |█████████████████████▍          | 12.6MB 11.0MB/s eta 0:00:01    72% |███████████████████████         | 13.6MB 10.3MB/s eta 0:00:01    74% |███████████████████████▊        | 14.0MB 8.8MB/s eta 0:00:01    76% |████████████████████████▌       | 14.5MB 12.1MB/s eta 0:00:01    81% |██████████████████████████      | 15.4MB 9.8MB/s eta 0:00:01    83% |██████████████████████████▉     | 15.8MB 8.8MB/s eta 0:00:01    88% |████████████████████████████▍   | 16.7MB 10.5MB/s eta 0:00:01    90% |█████████████████████████████   | 17.2MB 8.4MB/s eta 0:00:01    95% |██████████████████████████████▊ | 18.1MB 8.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: colour==0.1.5 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (0.1.5)\n",
      "Collecting cvxpy==1.0.3 (from -r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/59/2613468ffbbe3a818934d06b81b9f4877fe054afbf4f99d2f43f398a0b34/cvxpy-1.0.3.tar.gz (880kB)\n",
      "\u001b[K    100% |████████████████████████████████| 880kB 7.3MB/s eta 0:00:01    75% |████████████████████████▏       | 665kB 10.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cycler==0.10.0 in /opt/conda/lib/python3.6/site-packages/cycler-0.10.0-py3.6.egg (from -r requirements.txt (line 4)) (0.10.0)\n",
      "Collecting numpy==1.17.0 (from -r requirements.txt (line 5))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/b9/bda9781f0a74b90ebd2e046fde1196182900bd4a8e1ea503d3ffebc50e7c/numpy-1.17.0-cp36-cp36m-manylinux1_x86_64.whl (20.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 20.4MB 786kB/s ta 0:00:011  1% |▍                               | 256kB 9.5MB/s eta 0:00:03    3% |█▏                              | 768kB 9.6MB/s eta 0:00:03    6% |██                              | 1.3MB 10.1MB/s eta 0:00:02    8% |██▊                             | 1.8MB 10.7MB/s eta 0:00:02    11% |███▌                            | 2.2MB 8.6MB/s eta 0:00:03    18% |█████▊                          | 3.7MB 9.4MB/s eta 0:00:02    20% |██████▌                         | 4.2MB 10.6MB/s eta 0:00:02    22% |███████▎                        | 4.6MB 10.0MB/s eta 0:00:02    24% |████████                        | 5.1MB 9.0MB/s eta 0:00:02    29% |█████████▍                      | 6.0MB 7.2MB/s eta 0:00:03    34% |███████████                     | 7.0MB 10.6MB/s eta 0:00:02    36% |███████████▊                    | 7.4MB 10.5MB/s eta 0:00:02    38% |████████████▍                   | 7.9MB 9.5MB/s eta 0:00:02    45% |██████████████▋                 | 9.3MB 12.6MB/s eta 0:00:01    50% |████████████████                | 10.2MB 10.0MB/s eta 0:00:02    52% |████████████████▉               | 10.7MB 10.8MB/s eta 0:00:01    59% |███████████████████▏            | 12.2MB 10.0MB/s eta 0:00:01    61% |███████████████████▉            | 12.6MB 7.2MB/s eta 0:00:02    64% |████████████████████▌           | 13.1MB 9.5MB/s eta 0:00:01    66% |█████████████████████▏          | 13.5MB 7.9MB/s eta 0:00:01    70% |██████████████████████▋         | 14.4MB 9.4MB/s eta 0:00:01    72% |███████████████████████▎        | 14.8MB 9.1MB/s eta 0:00:01    74% |████████████████████████        | 15.3MB 9.6MB/s eta 0:00:01    81% |██████████████████████████      | 16.6MB 8.8MB/s eta 0:00:01    85% |███████████████████████████▍    | 17.4MB 8.9MB/s eta 0:00:01    89% |████████████████████████████▊   | 18.3MB 11.2MB/s eta 0:00:01    91% |█████████████████████████████▍  | 18.7MB 8.7MB/s eta 0:00:01    93% |██████████████████████████████  | 19.1MB 8.4MB/s eta 0:00:01    95% |██████████████████████████████▋ | 19.5MB 7.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==0.18.1 (from -r requirements.txt (line 6))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/09/e66eb844daba8680ddff26335d5b4fead77f60f957678243549a8dd4830d/pandas-0.18.1.tar.gz (7.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 7.3MB 3.4MB/s ta 0:00:011   6% |██                              | 471kB 18.5MB/s eta 0:00:01    18% |██████                          | 1.4MB 18.9MB/s eta 0:00:01    32% |██████████▍                     | 2.4MB 17.3MB/s eta 0:00:01    38% |████████████▎                   | 2.8MB 7.2MB/s eta 0:00:01    66% |█████████████████████▍          | 4.9MB 11.9MB/s eta 0:00:01    85% |███████████████████████████▏    | 6.2MB 10.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plotly==2.2.3 (from -r requirements.txt (line 7))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/a6/8214b6564bf4ace9bec8a26e7f89832792be582c042c47c912d3201328a0/plotly-2.2.3.tar.gz (1.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.1MB 6.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyparsing==2.2.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 8)) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil==2.6.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 9)) (2.6.1)\n",
      "Requirement already satisfied: pytz==2017.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 10)) (2017.3)\n",
      "Requirement already satisfied: requests==2.18.4 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 11)) (2.18.4)\n",
      "Collecting scipy==1.0.0 (from -r requirements.txt (line 12))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/5e/caa01ba7be11600b6a9d39265440d7b3be3d69206da887c42bef049521f2/scipy-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (50.0MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K    100% |████████████████████████████████| 50.0MB 874kB/s eta 0:00:01  0% |▏                               | 256kB 6.3MB/s eta 0:00:08    1% |▍                               | 665kB 8.7MB/s eta 0:00:06    2% |▊                               | 1.1MB 8.9MB/s eta 0:00:06    2% |█                               | 1.5MB 7.9MB/s eta 0:00:07    3% |█▎                              | 1.9MB 8.9MB/s eta 0:00:06    4% |█▌                              | 2.4MB 9.3MB/s eta 0:00:06    10% |███▌                            | 5.5MB 8.8MB/s eta 0:00:06    12% |████                            | 6.4MB 8.0MB/s eta 0:00:06    13% |████▍                           | 6.8MB 11.7MB/s eta 0:00:04    14% |████▋                           | 7.3MB 10.7MB/s eta 0:00:04    15% |█████                           | 7.7MB 10.7MB/s eta 0:00:04    16% |█████▏                          | 8.1MB 8.3MB/s eta 0:00:06    18% |█████▉                          | 9.1MB 10.8MB/s eta 0:00:04    19% |██████                          | 9.5MB 12.2MB/s eta 0:00:04    19% |██████▎                         | 9.9MB 10.3MB/s eta 0:00:04    20% |██████▋                         | 10.4MB 7.5MB/s eta 0:00:06    22% |███████▏                        | 11.2MB 9.0MB/s eta 0:00:05    23% |███████▌                        | 11.7MB 7.4MB/s eta 0:00:06    24% |███████▊                        | 12.1MB 7.3MB/s eta 0:00:06    25% |████████                        | 12.5MB 22.9MB/s eta 0:00:02    26% |████████▋                       | 13.5MB 9.1MB/s eta 0:00:05    27% |█████████                       | 13.9MB 12.1MB/s eta 0:00:03    28% |█████████▏                      | 14.3MB 10.3MB/s eta 0:00:04    30% |█████████▊                      | 15.2MB 8.0MB/s eta 0:00:05    32% |██████████▌                     | 16.5MB 9.4MB/s eta 0:00:04    33% |██████████▉                     | 16.9MB 10.9MB/s eta 0:00:04    35% |███████████▎                    | 17.7MB 9.8MB/s eta 0:00:04    36% |███████████▋                    | 18.2MB 7.2MB/s eta 0:00:05    37% |███████████▉                    | 18.6MB 8.3MB/s eta 0:00:04    38% |████████████▏                   | 19.1MB 13.8MB/s eta 0:00:03    40% |████████████▉                   | 20.0MB 14.6MB/s eta 0:00:03    41% |█████████████▏                  | 20.6MB 7.8MB/s eta 0:00:04    42% |█████████████▋                  | 21.3MB 10.5MB/s eta 0:00:03    43% |██████████████                  | 21.8MB 18.2MB/s eta 0:00:02    44% |██████████████▏                 | 22.2MB 8.4MB/s eta 0:00:04    45% |██████████████▍                 | 22.5MB 7.7MB/s eta 0:00:04    45% |██████████████▊                 | 22.9MB 10.9MB/s eta 0:00:03    46% |███████████████                 | 23.4MB 9.2MB/s eta 0:00:03    47% |███████████████▏                | 23.8MB 7.2MB/s eta 0:00:04    48% |███████████████▌                | 24.2MB 7.7MB/s eta 0:00:04    49% |███████████████▊                | 24.6MB 7.4MB/s eta 0:00:04    50% |████████████████                | 25.0MB 9.0MB/s eta 0:00:03    50% |████████████████▎               | 25.4MB 11.4MB/s eta 0:00:03    51% |████████████████▋               | 26.0MB 16.9MB/s eta 0:00:02    52% |████████████████▉               | 26.4MB 9.9MB/s eta 0:00:03    53% |█████████████████▏              | 26.8MB 10.6MB/s eta 0:00:03    54% |█████████████████▌              | 27.3MB 8.6MB/s eta 0:00:03    55% |█████████████████▊              | 27.7MB 8.7MB/s eta 0:00:03    57% |██████████████████▋             | 29.0MB 11.6MB/s eta 0:00:02    58% |██████████████████▉             | 29.5MB 9.8MB/s eta 0:00:03    59% |███████████████████▏            | 29.9MB 10.6MB/s eta 0:00:02    60% |███████████████████▍            | 30.3MB 7.7MB/s eta 0:00:03    61% |███████████████████▋            | 30.7MB 9.9MB/s eta 0:00:02    62% |████████████████████            | 31.4MB 9.4MB/s eta 0:00:02    63% |████████████████████▍           | 31.9MB 9.3MB/s eta 0:00:02    64% |████████████████████▋           | 32.3MB 7.0MB/s eta 0:00:03    65% |█████████████████████           | 32.8MB 9.3MB/s eta 0:00:02    66% |█████████████████████▎          | 33.2MB 8.5MB/s eta 0:00:02    67% |█████████████████████▌          | 33.6MB 9.9MB/s eta 0:00:02    68% |██████████████████████          | 34.5MB 7.0MB/s eta 0:00:03    69% |██████████████████████▎         | 34.9MB 8.1MB/s eta 0:00:02    70% |██████████████████████▌         | 35.3MB 10.9MB/s eta 0:00:02    71% |██████████████████████▉         | 35.7MB 5.8MB/s eta 0:00:03    72% |███████████████████████         | 36.1MB 7.6MB/s eta 0:00:02    73% |███████████████████████▋        | 37.0MB 11.8MB/s eta 0:00:02    75% |████████████████████████▎       | 37.9MB 9.7MB/s eta 0:00:02    76% |████████████████████████▋       | 38.5MB 5.1MB/s eta 0:00:03��█████████████████████       | 38.9MB 60.6MB/s eta 0:00:01    78% |█████████████████████████       | 39.2MB 4.9MB/s eta 0:00:03    79% |█████████████████████████▎      | 39.5MB 6.2MB/s eta 0:00:02    79% |█████████████████████████▌      | 39.9MB 11.2MB/s eta 0:00:01    80% |█████████████████████████▊      | 40.2MB 5.3MB/s eta 0:00:02    81% |██████████████████████████      | 40.7MB 9.8MB/s eta 0:00:01    82% |██████████████████████████▌     | 41.5MB 23.6MB/s eta 0:00:01    84% |███████████████████████████▏    | 42.4MB 19.2MB/s eta 0:00:01    86% |███████████████████████████▊    | 43.3MB 19.3MB/s eta 0:00:01    88% |████████████████████████████▍   | 44.4MB 20.4MB/s eta 0:00:01    92% |█████████████████████████████▋  | 46.2MB 21.4MB/s eta 0:00:01    94% |██████████████████████████████▎ | 47.3MB 23.6MB/s eta 0:00:01    96% |███████████████████████████████ | 48.3MB 18.1MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn==0.19.1 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 13)) (0.19.1)\n",
      "Requirement already satisfied: six==1.11.0 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 14)) (1.11.0)\n",
      "Collecting tables==3.3.0 (from -r requirements.txt (line 15))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/09/e7/72ca83c7bd75db94c23fcaf58debe1be5e9842376c630793e23765cab44b/tables-3.3.0-cp36-cp36m-manylinux1_x86_64.whl (4.6MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.6MB 8.7MB/s eta 0:00:01   9% |███                             | 430kB 20.6MB/s eta 0:00:01    50% |████████████████                | 2.3MB 17.6MB/s eta 0:00:01    71% |███████████████████████         | 3.3MB 21.5MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm==4.19.5 (from -r requirements.txt (line 16))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/3c/341b4fa23cb3abc335207dba057c790f3bb329f6757e1fcd5d347bcf8308/tqdm-4.19.5-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 7.9MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting zipline===1.2.0 (from -r requirements.txt (line 17))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d3/689f2a940478b82ac57c751a40460598221fd82b0449a7a8f7eef47a3bcc/zipline-1.2.0.tar.gz (659kB)\n",
      "\u001b[K    100% |████████████████████████████████| 665kB 13.4MB/s ta 0:00:01    57% |██████████████████▍             | 378kB 22.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting SQLAlchemy==1.3.0 (from -r requirements.txt (line 18))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/9e/5eb467ed50cdd8e88b808a7e65045020fa12b3b9c2ab51de0f452d269d4d/SQLAlchemy-1.3.0.tar.gz (5.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.9MB 6.2MB/s eta 0:00:01 0% |▎                               | 40kB 9.3MB/s eta 0:00:01    17% |█████▋                          | 1.0MB 21.2MB/s eta 0:00:01    62% |███████████████████▉            | 3.6MB 16.8MB/s eta 0:00:01    76% |████████████████████████▍       | 4.5MB 15.9MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=1.4.0 in /opt/conda/lib/python3.6/site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (2.1.0)\n",
      "Requirement already satisfied: seaborn>=0.6.0 in /opt/conda/lib/python3.6/site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in /opt/conda/lib/python3.6/site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (0.8.0)\n",
      "Requirement already satisfied: IPython>=3.2.3 in /opt/conda/lib/python3.6/site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (6.5.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting osqp (from cvxpy==1.0.3->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/82/b0693a167e4b9b5e94f4988f6df3d7866e9e41a316a58f1053dd21370f1a/osqp-0.6.2.post0-cp36-cp36m-manylinux1_x86_64.whl (211kB)\n",
      "\u001b[K    100% |████████████████████████████████| 215kB 15.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting ecos>=2 (from cvxpy==1.0.3->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/ed/d131ff51f3a8f73420eb1191345eb49f269f23cadef515172e356018cde3/ecos-2.0.7.post1-cp36-cp36m-manylinux1_x86_64.whl (147kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 13.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting scs>=1.1.3 (from cvxpy==1.0.3->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/72/33be87cce255d4e9dbbfef547e9fd6ec7ee94d0d0910bb2b13badea3fbbe/scs-2.1.2.tar.gz (3.5MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.6MB 7.9MB/s eta 0:00:01   4% |█▋                              | 174kB 16.9MB/s eta 0:00:01    32% |██████████▌                     | 1.2MB 19.9MB/s eta 0:00:01    59% |███████████████████             | 2.1MB 17.3MB/s eta 0:00:01    88% |████████████████████████████▎   | 3.1MB 19.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess (from cvxpy==1.0.3->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8f/dc/426a82723c460cfab653ebb717590103d6e38cebc9d1f599b0898915ac1d/multiprocess-0.70.11.1-py36-none-any.whl (101kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 9.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fastcache in /opt/conda/lib/python3.6/site-packages (from cvxpy==1.0.3->-r requirements.txt (line 3)) (1.0.2)\n",
      "Requirement already satisfied: toolz in /opt/conda/lib/python3.6/site-packages (from cvxpy==1.0.3->-r requirements.txt (line 3)) (0.8.2)\n",
      "Requirement already satisfied: decorator>=4.0.6 in /opt/conda/lib/python3.6/site-packages (from plotly==2.2.3->-r requirements.txt (line 7)) (4.0.11)\n",
      "Requirement already satisfied: nbformat>=4.2 in /opt/conda/lib/python3.6/site-packages (from plotly==2.2.3->-r requirements.txt (line 7)) (4.4.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 11)) (3.0.4)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 11)) (2.6)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 11)) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 11)) (2019.11.28)\n",
      "Requirement already satisfied: numexpr>=2.5.2 in /opt/conda/lib/python3.6/site-packages (from tables==3.3.0->-r requirements.txt (line 15)) (2.6.4)\n",
      "Requirement already satisfied: pip>=7.1.0 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 17)) (18.1)\n",
      "Requirement already satisfied: setuptools>18.0 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 17)) (38.4.0)\n",
      "Collecting Logbook>=0.12.5 (from zipline===1.2.0->-r requirements.txt (line 17))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2f/d9/16ac346f7c0102835814cc9e5b684aaadea101560bb932a2403bd26b2320/Logbook-1.5.3.tar.gz (85kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 6.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting requests-file>=1.4.1 (from zipline===1.2.0->-r requirements.txt (line 17))\n",
      "  Downloading https://files.pythonhosted.org/packages/77/86/cdb5e8eaed90796aa83a6d9f75cfbd37af553c47a291cd47bc410ef9bdb2/requests_file-1.5.1-py2.py3-none-any.whl\n",
      "Collecting pandas-datareader<0.6,>=0.2.1 (from zipline===1.2.0->-r requirements.txt (line 17))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/c5/cc720f531bbde0efeab940de400d0fcc95e87770a3abcd7f90d6d52a3302/pandas_datareader-0.5.0-py2.py3-none-any.whl (74kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 8.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: patsy>=0.4.0 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 17)) (0.4.1)\n",
      "Requirement already satisfied: Cython>=0.25.2 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 17)) (0.29.7)\n",
      "Collecting cyordereddict>=0.2.2 (from zipline===1.2.0->-r requirements.txt (line 17))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/1a/364cbfd927be1b743c7f0a985a7f1f7e8a51469619f9fefe4ee9240ba210/cyordereddict-1.0.0.tar.gz (138kB)\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 15.0MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting bottleneck>=1.0.0 (from zipline===1.2.0->-r requirements.txt (line 17))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5b/08/278c6ee569458e168096f6b51019cc1c81c288da3d1026a22ee2ccead102/Bottleneck-1.3.2.tar.gz (88kB)\n",
      "\u001b[K    100% |████████████████████████████████| 92kB 9.0MB/s ta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting contextlib2>=0.4.0 (from zipline===1.2.0->-r requirements.txt (line 17))\n",
      "  Downloading https://files.pythonhosted.org/packages/85/60/370352f7ef6aa96c52fb001831622f50f923c1d575427d021b8ab3311236/contextlib2-0.6.0.post1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: networkx<2.0,>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 17)) (1.11)\n",
      "Collecting bcolz<1,>=0.12.1 (from zipline===1.2.0->-r requirements.txt (line 17))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/8b/1ffa01f872cac36173c5eb95b58c01040d8d25f1b242c48577f4104cd3ab/bcolz-0.12.1.tar.gz (622kB)\n",
      "\u001b[K    100% |████████████████████████████████| 624kB 13.3MB/s ta 0:00:01    39% |████████████▋                   | 245kB 24.2MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click>=4.0.0 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 17)) (6.7)\n",
      "Collecting multipledispatch>=0.4.8 (from zipline===1.2.0->-r requirements.txt (line 17))\n",
      "  Downloading https://files.pythonhosted.org/packages/89/79/429ecef45fd5e4504f7474d4c3c3c4668c267be3370e4c2fd33e61506833/multipledispatch-0.6.0-py3-none-any.whl\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 17)) (1.0)\n",
      "Requirement already satisfied: Mako>=1.0.1 in /opt/conda/lib/python3.6/site-packages/Mako-1.0.7-py3.6.egg (from zipline===1.2.0->-r requirements.txt (line 17)) (1.0.7)\n",
      "Collecting alembic>=0.7.7 (from zipline===1.2.0->-r requirements.txt (line 17))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/29/ed5c134aba874053859ba3e8d4705b4a5c1b66156deabc26cbe643e83f2e/alembic-1.5.7-py2.py3-none-any.whl (159kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 14.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting sortedcontainers>=1.4.4 (from zipline===1.2.0->-r requirements.txt (line 17))\n",
      "  Downloading https://files.pythonhosted.org/packages/20/4d/a7046ae1a1a4cc4e9bbed194c387086f06b25038be596543d026946330c9/sortedcontainers-2.3.0-py2.py3-none-any.whl\n",
      "Collecting intervaltree>=2.1.0 (from zipline===1.2.0->-r requirements.txt (line 17))\n",
      "  Downloading https://files.pythonhosted.org/packages/50/fb/396d568039d21344639db96d940d40eb62befe704ef849b27949ded5c3bb/intervaltree-3.1.0.tar.gz\n",
      "Collecting lru-dict>=1.1.4 (from zipline===1.2.0->-r requirements.txt (line 17))\n",
      "  Downloading https://files.pythonhosted.org/packages/68/ea/997af58d4e6da019ad825a412f93081d9df67e9dda11cfb026a3d7cd0b6c/lru-dict-1.1.7.tar.gz\n",
      "Collecting empyrical>=0.4.2 (from zipline===1.2.0->-r requirements.txt (line 17))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/43/1b997c21411c6ab7c96dc034e160198272c7a785aeea7654c9bcf98bec83/empyrical-0.5.5.tar.gz (52kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 1.1MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.10.2)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (4.3.2)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.15 in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (1.0.15)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.1.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.7.4)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (4.3.1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: simplegeneric>0.8 in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.8.1)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (2.2.0)\n",
      "Collecting qdldl (from osqp->cvxpy==1.0.3->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/a3/db0e7c9fec5387dc33cbd2819329c141ba76497148aa9fab4bd1a7c2a279/qdldl-0.1.5.post0.tar.gz (69kB)\n",
      "\u001b[K    100% |████████████████████████████████| 71kB 9.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting dill>=0.3.3 (from multiprocess->cvxpy==1.0.3->-r requirements.txt (line 3))\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/52/d6/79f40d230895fa1ce3b6af0d22e0ac79c65175dc069c194b79cc8e05a033/dill-0.3.3-py2.py3-none-any.whl (81kB)\n",
      "\u001b[K    100% |████████████████████████████████| 81kB 8.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipython-genutils in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 7)) (2.6.0)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.6/site-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 7)) (4.4.0)\n",
      "Collecting requests-ftp (from pandas-datareader<0.6,>=0.2.1->zipline===1.2.0->-r requirements.txt (line 17))\n",
      "  Downloading https://files.pythonhosted.org/packages/3d/ca/14b2ad1e93b5195eeaf56b86b7ecfd5ea2d5754a68d17aeb1e5b9f95b3cf/requests-ftp-0.3.1.tar.gz\n",
      "Collecting python-editor>=0.3 (from alembic>=0.7.7->zipline===1.2.0->-r requirements.txt (line 17))\n",
      "  Downloading https://files.pythonhosted.org/packages/c6/d3/201fc3abe391bbae6606e6f1d598c15d367033332bd54352b12f35513717/python_editor-1.0.4-py3-none-any.whl\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from prompt-toolkit<2.0.0,>=1.0.15->IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.1.7)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.5.2)\n",
      "Building wheels for collected packages: alphalens, cvxpy, pandas, plotly, zipline, SQLAlchemy, scs, Logbook, cyordereddict, bottleneck, bcolz, intervaltree, lru-dict, empyrical, qdldl, requests-ftp\n",
      "  Running setup.py bdist_wheel for alphalens ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/77/1e/9a/223b4c94d7f564f25d94b48ca5b9c53e3034016ece3fd8c8c1\n",
      "  Running setup.py bdist_wheel for cvxpy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2b/60/0b/0c2596528665e21d698d6f84a3406c52044c7b4ca6ac737cf3\n",
      "  Running setup.py bdist_wheel for pandas ... \u001b[?25l|"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cvxpy as cvx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import project_tests\n",
    "import project_helper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Bundle\n",
    "We'll be using Zipline to handle our data. We've created a end of day data bundle for this project. Run the cell below to register this data bundle in zipline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import project_helper\n",
    "from zipline.data import bundles\n",
    "\n",
    "os.environ['ZIPLINE_ROOT'] = os.path.join(os.getcwd(), '..', '..', 'data', 'project_4_eod')\n",
    "\n",
    "ingest_func = bundles.csvdir.csvdir_equities(['daily'], project_helper.EOD_BUNDLE_NAME)\n",
    "bundles.register(project_helper.EOD_BUNDLE_NAME, ingest_func)\n",
    "\n",
    "print('Data Registered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pipeline Engine\n",
    "We'll be using Zipline's pipeline package to access our data for this project. To use it, we must build a pipeline engine. Run the cell below to build the engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.factors import AverageDollarVolume\n",
    "from zipline.utils.calendars import get_calendar\n",
    "\n",
    "\n",
    "universe = AverageDollarVolume(window_length=120).top(500) \n",
    "trading_calendar = get_calendar('NYSE') \n",
    "bundle_data = bundles.load(project_helper.EOD_BUNDLE_NAME)\n",
    "engine = project_helper.build_pipeline_engine(bundle_data, trading_calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "With the pipeline engine built, let's get the stocks at the end of the period in the universe we're using. We'll use these tickers to generate the returns data for the our risk model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_end_date = pd.Timestamp('2016-01-05', tz='UTC')\n",
    "\n",
    "universe_tickers = engine\\\n",
    "    .run_pipeline(\n",
    "        Pipeline(screen=universe),\n",
    "        universe_end_date,\n",
    "        universe_end_date)\\\n",
    "    .index.get_level_values(1)\\\n",
    "    .values.tolist()\n",
    "    \n",
    "universe_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Returns\n",
    "Not that we have our pipeline built, let's access the returns data. We'll start by building a data portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.data.data_portal import DataPortal\n",
    "\n",
    "\n",
    "data_portal = DataPortal(\n",
    "    bundle_data.asset_finder,\n",
    "    trading_calendar=trading_calendar,\n",
    "    first_trading_day=bundle_data.equity_daily_bar_reader.first_trading_day,\n",
    "    equity_minute_reader=None,\n",
    "    equity_daily_reader=bundle_data.equity_daily_bar_reader,\n",
    "    adjustment_reader=bundle_data.adjustment_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the code easier to read, we've built the helper function `get_pricing` to get the pricing from the data portal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pricing(data_portal, trading_calendar, assets, start_date, end_date, field='close'):\n",
    "    end_dt = pd.Timestamp(end_date.strftime('%Y-%m-%d'), tz='UTC', offset='C')\n",
    "    start_dt = pd.Timestamp(start_date.strftime('%Y-%m-%d'), tz='UTC', offset='C')\n",
    "\n",
    "    end_loc = trading_calendar.closes.index.get_loc(end_dt)\n",
    "    start_loc = trading_calendar.closes.index.get_loc(start_dt)\n",
    "\n",
    "    return data_portal.get_history_window(\n",
    "        assets=assets,\n",
    "        end_dt=end_dt,\n",
    "        bar_count=end_loc - start_loc,\n",
    "        frequency='1d',\n",
    "        field=field,\n",
    "        data_frequency='daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's get returns data for our risk model using the `get_pricing` function. For this model, we'll be looking back to 5 years of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_year_returns = \\\n",
    "    get_pricing(\n",
    "        data_portal,\n",
    "        trading_calendar,\n",
    "        universe_tickers,\n",
    "        universe_end_date - pd.DateOffset(years=5),\n",
    "        universe_end_date)\\\n",
    "    .pct_change()[1:].fillna(0)\n",
    "\n",
    "five_year_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Risk Model\n",
    "It's time to build the risk model. You'll be creating a statistical risk model using PCA. So, the first thing is building the PCA model.\n",
    "## Fit PCA\n",
    "Implement `fit_pca` to fit a PCA model to the returns data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def fit_pca(returns, num_factor_exposures, svd_solver):\n",
    "    \"\"\"\n",
    "    Fit PCA model with returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    num_factor_exposures : int\n",
    "        Number of factors for PCA\n",
    "    svd_solver: str\n",
    "        The solver to use for the PCA model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pca : PCA\n",
    "        Model fit to returns\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    #define n_components\n",
    "    k_components=num_factor_exposures #k\n",
    "    #define pca function\n",
    "    pca = PCA(n_components=k_components, svd_solver = svd_solver)\n",
    "    #run pca fit\n",
    "    pca.fit(returns)\n",
    "    #return the PCA\n",
    "    \n",
    "    return pca\n",
    "\n",
    "\n",
    "project_tests.test_fit_pca(fit_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's see what the model looks like. First, we'll look at the PCA components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_factor_exposures = 20\n",
    "pca = fit_pca(five_year_returns, num_factor_exposures, 'full')\n",
    "\n",
    "#pca.components_\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the PCA's percent of variance explained by each factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(num_factor_exposures), pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that the first factor dominates. The precise definition of each factor in a latent model is unknown, however we can guess at the likely interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Betas\n",
    "Implement `factor_betas` to get the factor betas from the PCA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_betas(pca, factor_beta_indices, factor_beta_columns):\n",
    "    \"\"\"\n",
    "    Get the factor betas from the PCA model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pca : PCA\n",
    "        Model fit to returns\n",
    "    factor_beta_indices : 1 dimensional Ndarray\n",
    "        Factor beta indices\n",
    "    factor_beta_columns : 1 dimensional Ndarray\n",
    "        Factor beta columns\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    factor_betas : DataFrame\n",
    "        Factor betas\n",
    "    \"\"\"\n",
    "    assert len(factor_beta_indices.shape) == 1\n",
    "    assert len(factor_beta_columns.shape) == 1\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    B = pd.DataFrame(pca.components_.T, index = factor_beta_indices, columns = factor_beta_columns)\n",
    "    \n",
    "    return B\n",
    "\n",
    "\n",
    "project_tests.test_factor_betas(factor_betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's view the factor betas from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_model = {}\n",
    "risk_model['factor_betas'] = factor_betas(pca, five_year_returns.columns.values, np.arange(num_factor_exposures))\n",
    "\n",
    "risk_model['factor_betas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Returns\n",
    "Implement `factor_returns` to get the factor returns from the PCA model using the returns data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_returns(pca, returns, factor_return_indices, factor_return_columns):\n",
    "    \"\"\"\n",
    "    Get the factor returns from the PCA model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pca : PCA\n",
    "        Model fit to returns\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    factor_return_indices : 1 dimensional Ndarray\n",
    "        Factor return indices\n",
    "    factor_return_columns : 1 dimensional Ndarray\n",
    "        Factor return columns\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    factor_returns : DataFrame\n",
    "        Factor returns\n",
    "    \"\"\"\n",
    "    assert len(factor_return_indices.shape) == 1\n",
    "    assert len(factor_return_columns.shape) == 1\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    # the pca transform is used to transform the returns to the new basis\n",
    "    factor_returns = pd.DataFrame(pca.transform(returns), index = factor_return_indices, columns = factor_return_columns)\n",
    "    #print (factor_return_indices)\n",
    "    #print (factor_return_columns)\n",
    "    #print (factor_returns.shape)\n",
    "    return factor_returns\n",
    "\n",
    "\n",
    "project_tests.test_factor_returns(factor_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's see what these factor returns looks like over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_model['factor_returns'] = factor_returns(\n",
    "    pca,\n",
    "    five_year_returns,\n",
    "    five_year_returns.index,\n",
    "    np.arange(num_factor_exposures))\n",
    "\n",
    "risk_model['factor_returns'].cumsum().plot(legend=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Covariance Matrix\n",
    "Implement `factor_cov_matrix` to get the factor covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_cov_matrix(factor_returns, ann_factor):\n",
    "    \"\"\"\n",
    "    Get the factor covariance matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    factor_returns : DataFrame\n",
    "        Factor returns\n",
    "    ann_factor : int\n",
    "        Annualization factor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    factor_cov_matrix : 2 dimensional Ndarray\n",
    "        Factor covariance matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return np.diag(factor_returns.var(axis=0, ddof=1)*ann_factor)\n",
    "\n",
    "\n",
    "project_tests.test_factor_cov_matrix(factor_cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_factor = 252\n",
    "risk_model['factor_cov_matrix'] = factor_cov_matrix(risk_model['factor_returns'], ann_factor)\n",
    "\n",
    "print(risk_model['factor_cov_matrix'].shape)\n",
    "risk_model['factor_cov_matrix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idiosyncratic Variance Matrix\n",
    "Implement `idiosyncratic_var_matrix` to get the idiosyncratic variance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idiosyncratic_var_matrix(returns, factor_returns, factor_betas, ann_factor):\n",
    "    \"\"\"\n",
    "    Get the idiosyncratic variance matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    factor_returns : DataFrame\n",
    "        Factor returns\n",
    "    factor_betas : DataFrame\n",
    "        Factor betas\n",
    "    ann_factor : int\n",
    "        Annualization factor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    idiosyncratic_var_matrix : DataFrame\n",
    "        Idiosyncratic variance matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    #S = r - Bf\n",
    "    # r = T-1296 x N-490, B = N-490 x K -20, f = T-1296 x K-20, ann_factor = 252\n",
    "    \n",
    "    common_returns = pd.DataFrame(np.dot(factor_returns,factor_betas.T), index = returns.index, columns = returns.columns)\n",
    "    \n",
    "    residual_returns = returns - common_returns\n",
    "    \n",
    "    idio_var_matrix = ann_factor*np.var(residual_returns)\n",
    "    \n",
    "    idio_dataframe = pd.DataFrame(np.diag(idio_var_matrix), index=returns.columns, columns=returns.columns)\n",
    "    \n",
    "    return idio_dataframe\n",
    "\n",
    "\n",
    "project_tests.test_idiosyncratic_var_matrix(idiosyncratic_var_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_model['idiosyncratic_var_matrix'] = idiosyncratic_var_matrix(five_year_returns, risk_model['factor_returns'], risk_model['factor_betas'], ann_factor)\n",
    "\n",
    "risk_model['idiosyncratic_var_matrix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idiosyncratic Variance Vector\n",
    "Implement `idiosyncratic_var_vector` to get the idiosyncratic variance Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idiosyncratic_var_vector(returns, idiosyncratic_var_matrix):\n",
    "    \"\"\"\n",
    "    Get the idiosyncratic variance vector\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    idiosyncratic_var_matrix : DataFrame\n",
    "        Idiosyncratic variance matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    idiosyncratic_var_vector : DataFrame\n",
    "        Idiosyncratic variance Vector\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return pd.DataFrame(np.diagonal(idiosyncratic_var_matrix), index=returns.columns)\n",
    "\n",
    "\n",
    "project_tests.test_idiosyncratic_var_vector(idiosyncratic_var_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_model['idiosyncratic_var_vector'] = idiosyncratic_var_vector(five_year_returns, risk_model['idiosyncratic_var_matrix'])\n",
    "\n",
    "risk_model['idiosyncratic_var_vector']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using the Risk Model\n",
    "Using the data we calculated in the risk model, implement `predict_portfolio_risk` to predict the portfolio risk using the formula $ \\sqrt{X^{T}(BFB^{T} + S)X} $ where:\n",
    "- $ X $ is the portfolio weights\n",
    "- $ B $ is the factor betas\n",
    "- $ F $ is the factor covariance matrix\n",
    "- $ S $ is the idiosyncratic variance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_portfolio_risk(factor_betas, factor_cov_matrix, idiosyncratic_var_matrix, weights):\n",
    "    \"\"\"\n",
    "    Get the predicted portfolio risk\n",
    "    \n",
    "    Formula for predicted portfolio risk is sqrt(X.T(BFB.T + S)X) where:\n",
    "      X is the portfolio weights\n",
    "      B is the factor betas\n",
    "      F is the factor covariance matrix\n",
    "      S is the idiosyncratic variance matrix\n",
    "    # r = T-1296 x N-490, B = N-490 x K -20, f = T-1296 x K-20, X= N-490 x 1\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    factor_betas : DataFrame\n",
    "        Factor betas\n",
    "    factor_cov_matrix : 2 dimensional Ndarray\n",
    "        Factor covariance matrix\n",
    "    idiosyncratic_var_matrix : DataFrame\n",
    "        Idiosyncratic variance matrix\n",
    "    weights : DataFrame\n",
    "        Portfolio weights\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predicted_portfolio_risk : float\n",
    "        Predicted portfolio risk\n",
    "    \"\"\"\n",
    "    assert len(factor_cov_matrix.shape) == 2\n",
    "    \n",
    "    #TODO: Implement function\n",
    "\n",
    "    \n",
    "    X = weights\n",
    "    B = factor_betas\n",
    "    F = factor_cov_matrix\n",
    "    S = idiosyncratic_var_matrix # N x N\n",
    "    common_plus_residual = np.dot(B,np.dot(F,B.T)) + S # BFB.T + S -> N x k dot t x k dot k x N\n",
    "    risk = np.sqrt(np.dot(X.T,(np.dot(common_plus_residual,X))))\n",
    "    #print(B)\n",
    "    #print (risk)\n",
    "    return risk.item()\n",
    "\n",
    "\n",
    "project_tests.test_predict_portfolio_risk(predict_portfolio_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's see what the portfolio risk would be if we had even weights across all stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = pd.DataFrame(np.repeat(1/len(universe_tickers), len(universe_tickers)), universe_tickers)\n",
    "\n",
    "predict_portfolio_risk(\n",
    "    risk_model['factor_betas'],\n",
    "    risk_model['factor_cov_matrix'],\n",
    "    risk_model['idiosyncratic_var_matrix'],\n",
    "    all_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Alpha Factors\n",
    "With the profile risk calculated, it's time to start working on the alpha factors. In this project, we'll create the following factors:\n",
    "- Momentum 1 Year Factor\n",
    "- Mean Reversion 5 Day Sector Neutral Factor\n",
    "- Mean Reversion 5 Day Sector Neutral Smoothed Factor\n",
    "- Overnight Sentiment Factor\n",
    "- Overnight Sentiment Smoothed Factor\n",
    "\n",
    "## Momentum 1 Year Factor\n",
    "Each factor will have a hypothesis that goes with it. For this factor, it is \"Higher past 12-month (252 days) returns are proportional to future return.\" Using that hypothesis, we've generated this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline.factors import Returns\n",
    "\n",
    "def momentum_1yr(window_length, universe, sector):\n",
    "    return Returns(window_length=window_length, mask=universe) \\\n",
    "        .demean(groupby=Sector()) \\\n",
    "        .rank() \\\n",
    "        .zscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Reversion 5 Day Sector Neutral Factor\n",
    "Now it's time for you to implement `mean_reversion_5day_sector_neutral` using the hypothesis \"Short-term outperformers(underperformers) compared to their sector will revert.\" Use the returns data from `universe`, demean using the sector data to partition, rank, then converted to a zscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline.factors import Returns\n",
    "\n",
    "def mean_reversion_5day_sector_neutral(window_length, universe, sector):\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Generate the mean reversion 5 day sector neutral factor\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    window_length : int\n",
    "        Returns window length\n",
    "    universe : Zipline Filter\n",
    "        Universe of stocks filter\n",
    "    sector : Zipline Classifier\n",
    "        Sector classifier\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    factor : Zipline Factor\n",
    "        Mean reversion 5 day sector neutral factor\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    \n",
    "    \n",
    "\n",
    "    return Returns(window_length=window_length, mask=universe).demean(groupby=Sector()).rank().zscore()\n",
    "\n",
    "\n",
    "\n",
    "project_tests.test_mean_reversion_5day_sector_neutral(mean_reversion_5day_sector_neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's see what some of the factor data looks like. For calculating factors, we'll be looking back 2 years.\n",
    "\n",
    "**Note:** _Going back 2 years falls on a day when the market is closed. Pipeline package doesn't handle start or end dates that don't fall on days when the market is open. To fix this, we went back 2 extra days to fall on the next day when the market is open._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_start_date = universe_end_date - pd.DateOffset(years=2, days=2)\n",
    "sector = project_helper.Sector()\n",
    "window_length = 5\n",
    "\n",
    "pipeline = Pipeline(screen=universe)\n",
    "pipeline.add(\n",
    "    mean_reversion_5day_sector_neutral(window_length, universe, sector),\n",
    "    'Mean_Reversion_5Day_Sector_Neutral')\n",
    "engine.run_pipeline(pipeline, factor_start_date, universe_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Reversion 5 Day Sector Neutral Smoothed Factor\n",
    "Taking the output of the previous factor, let's create a smoothed version. Implement `mean_reversion_5day_sector_neutral_smoothed` to generate a mean reversion 5 fay sector neutral smoothed factor. Call the `mean_reversion_5day_sector_neutral` function to get the unsmoothed factor, then use `SimpleMovingAverage` function to smooth it. You'll have to apply rank and zscore again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline.factors import SimpleMovingAverage\n",
    "\n",
    "def mean_reversion_5day_sector_neutral_smoothed(window_length, universe, sector):\n",
    "    \"\"\"\n",
    "    Generate the mean reversion 5 day sector neutral smoothed factor\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    window_length : int\n",
    "        Returns window length\n",
    "    universe : Zipline Filter\n",
    "        Universe of stocks filter\n",
    "    sector : Zipline Classifier\n",
    "        Sector classifier\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    factor : Zipline Factor\n",
    "        Mean reversion 5 day sector neutral smoothed factor\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    alpha_5day = mean_reversion_5day_sector_neutral(window_length, universe, sector)\n",
    "    # Use this factor as input into SimpleMovingAverage, with a window length of 5\n",
    "    factorSMA= (SimpleMovingAverage(inputs=[alpha_5day], window_length=window_length)).rank().zscore()\n",
    "   # # Also rank and zscore (don't need to de-mean by sector, s)\n",
    "    ##factor_SMA_zscore = (factorSMA.rank().zscore())\n",
    "    #factor = SimpleMovingAverage(inputs=[mean_reversion_5day_sector_neutral(window_length, universe, sector)],window_length=window_length).rank().zscore()\n",
    "    return factorSMA\n",
    "\n",
    "\n",
    "project_tests.test_mean_reversion_5day_sector_neutral_smoothed(mean_reversion_5day_sector_neutral_smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's see what some of the smoothed data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(screen=universe)\n",
    "pipeline.add(\n",
    "    mean_reversion_5day_sector_neutral_smoothed(5, universe, sector),\n",
    "    'Mean_Reversion_5Day_Sector_Neutral_Smoothed')\n",
    "engine.run_pipeline(pipeline, factor_start_date, universe_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overnight Sentiment Factor\n",
    "For this factor, were using the hypothesis from the paper [_Overnight Returns and Firm-Specific Investor Sentiment_](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline.data import USEquityPricing\n",
    "\n",
    "\n",
    "class CTO(Returns):\n",
    "    \"\"\"\n",
    "    Computes the overnight return, per hypothesis from\n",
    "    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554010\n",
    "    \"\"\"\n",
    "    inputs = [USEquityPricing.open, USEquityPricing.close]\n",
    "    \n",
    "    def compute(self, today, assets, out, opens, closes):\n",
    "        \"\"\"\n",
    "        The opens and closes matrix is 2 rows x N assets, with the most recent at the bottom.\n",
    "        As such, opens[-1] is the most recent open, and closes[0] is the earlier close\n",
    "        \"\"\"\n",
    "        out[:] = (opens[-1] - closes[0]) / closes[0]\n",
    "\n",
    "        \n",
    "class TrailingOvernightReturns(Returns):\n",
    "    \"\"\"\n",
    "    Sum of trailing 1m O/N returns\n",
    "    \"\"\"\n",
    "    window_safe = True\n",
    "    \n",
    "    def compute(self, today, asset_ids, out, cto):\n",
    "        out[:] = np.nansum(cto, axis=0)\n",
    "\n",
    "        \n",
    "def overnight_sentiment(cto_window_length, trail_overnight_returns_window_length, universe):\n",
    "    cto_out = CTO(mask=universe, window_length=cto_window_length)\n",
    "    return TrailingOvernightReturns(inputs=[cto_out], window_length=trail_overnight_returns_window_length) \\\n",
    "        .rank() \\\n",
    "        .zscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overnight Sentiment Smoothed Factor\n",
    "Just like the factor you implemented, we'll also smooth this factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overnight_sentiment_smoothed(cto_window_length, trail_overnight_returns_window_length, universe):\n",
    "    unsmoothed_factor = overnight_sentiment(cto_window_length, trail_overnight_returns_window_length, universe)\n",
    "    #rahul# mean_reversion_5day_sector_neutral_smoothed(window_length, universe, sector)\n",
    "    #rahul# factor_smoothed = SimpleMovingAverage(inputs=factor, window_length=5).rank().zscore()\n",
    "    return SimpleMovingAverage(inputs=[unsmoothed_factor], window_length=trail_overnight_returns_window_length) \\\n",
    "        .rank() \\\n",
    "        .zscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Factors to a single Pipeline\n",
    "With all the factor implementations done, let's add them to a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = AverageDollarVolume(window_length=120).top(500)\n",
    "sector = project_helper.Sector()\n",
    "\n",
    "pipeline = Pipeline(screen=universe)\n",
    "pipeline.add(\n",
    "    momentum_1yr(252, universe, sector),\n",
    "    'Momentum_1YR')\n",
    "pipeline.add(\n",
    "    mean_reversion_5day_sector_neutral(5, universe, sector),\n",
    "    'Mean_Reversion_5Day_Sector_Neutral')\n",
    "pipeline.add(\n",
    "    mean_reversion_5day_sector_neutral_smoothed(5, universe, sector),\n",
    "    'Mean_Reversion_5Day_Sector_Neutral_Smoothed')\n",
    "pipeline.add(\n",
    "    overnight_sentiment(2, 5, universe),\n",
    "    'Overnight_Sentiment')\n",
    "pipeline.add(\n",
    "    overnight_sentiment_smoothed(2, 5, universe),\n",
    "    'Overnight_Sentiment_Smoothed')\n",
    "all_factors = engine.run_pipeline(pipeline, factor_start_date, universe_end_date)\n",
    "\n",
    "all_factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Alpha Factors\n",
    "*Note:* _We're evaluating the alpha factors using delay of 1_\n",
    "## Get Pricing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alphalens as al\n",
    "\n",
    "assets = all_factors.index.levels[1].values.tolist()\n",
    "pricing = get_pricing(\n",
    "    data_portal,\n",
    "    trading_calendar,\n",
    "    assets,\n",
    "    factor_start_date,\n",
    "    universe_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format alpha factors and pricing for Alphalens\n",
    "In order to use a lot of the alphalens functions, we need to aligned the indices and convert the time to unix timestamp. In this next cell, we'll do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_factor_data = {\n",
    "    factor: al.utils.get_clean_factor_and_forward_returns(factor=factor_data, prices=pricing, periods=[1])\n",
    "    for factor, factor_data in all_factors.iteritems()}\n",
    "\n",
    "unixt_factor_data = {\n",
    "    factor: factor_data.set_index(pd.MultiIndex.from_tuples(\n",
    "        [(x.timestamp(), y) for x, y in factor_data.index.values],\n",
    "        names=['date', 'asset']))\n",
    "    for factor, factor_data in clean_factor_data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Analysis\n",
    "### Factor Returns\n",
    "Let's view the factor returns over time. We should be seeing it generally move up and to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_factor_returns = pd.DataFrame()\n",
    "\n",
    "for factor, factor_data in clean_factor_data.items():\n",
    "    ls_factor_returns[factor] = al.performance.factor_returns(factor_data).iloc[:, 0]\n",
    "\n",
    "(1+ls_factor_returns).cumprod().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis Points Per Day per Quantile\n",
    "It is not enough to look just at the factor weighted return. A good alpha is also monotonic in quantiles. Let's looks the basis points for the factor returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_factor_returns = pd.DataFrame()\n",
    "\n",
    "for factor, factor_data in unixt_factor_data.items():\n",
    "    qr_factor_returns[factor] = al.performance.mean_return_by_quantile(factor_data)[0].iloc[:, 0]\n",
    "\n",
    "(10000*qr_factor_returns).plot.bar(\n",
    "    subplots=True,\n",
    "    sharey=True,\n",
    "    layout=(4,2),\n",
    "    figsize=(14, 14),\n",
    "    legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?\n",
    "\n",
    "- None of these alphas are **strictly monotonic**; this should lead you to question why this is? Further research and refinement of the alphas needs to be done. What is it about these alphas that leads to the highest ranking stocks in all alphas except MR 5D smoothed to *not* perform the best.\n",
    "- The majority of the return is coming from the **short side** in all these alphas. The negative return in quintile 1 is very large in all alphas. This could also a cause for concern becuase when you short stocks, you need to locate the short; shorts can be expensive or not available at all.\n",
    "- If you look at the magnitude of the return spread (i.e., Q1 minus Q5), we are working with daily returns in the 0.03%, i.e., **3 basis points**, neighborhood *before all transaction costs, shorting costs, etc.*. Assuming 252 days in a year, that's 7.56% return annualized. Transaction costs may cut this in half. As such, it should be clear that these alphas can only survive in an institutional setting and that leverage will likely need to be applied in order to achieve an attractive return.\n",
    "\n",
    "## Turnover Analysis\n",
    "\n",
    "Without doing a full and formal backtest, we can analyze how stable the alphas are over time. Stability in this sense means that from period to period, the alpha ranks do not change much. Since trading is costly, we always prefer, all other things being equal, that the ranks do not change significantly per period. We can measure this with the **factor rank autocorrelation (FRA)**.\n",
    "\n",
    "[alphalens.performance.factor_rank_autocorrelation](https://quantopian.github.io/alphalens/alphalens.html?highlight=factor_rank_autocorrelation#alphalens.performance.factor_rank_autocorrelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_FRA = pd.DataFrame()\n",
    "\n",
    "for factor, factor_data in unixt_factor_data.items():\n",
    "    ls_FRA[factor] = al.performance.factor_rank_autocorrelation(factor_data)\n",
    "\n",
    "ls_FRA.plot(title=\"Factor Rank Autocorrelation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharpe Ratio of the Alphas\n",
    "\n",
    "The last analysis we'll do on the factors will be sharpe ratio. Implement `sharpe_ratio` to calculate the sharpe ratio of factor returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio(factor_returns, annualization_factor):\n",
    "    \"\"\"\n",
    "    Get the sharpe ratio for each factor for the entire period\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    factor_returns : DataFrame\n",
    "        Factor returns for each factor and date\n",
    "    annualization_factor: float\n",
    "        Annualization Factor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sharpe_ratio : Pandas Series of floats\n",
    "        Sharpe ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    sharpe_ratio = annualization_factor * factor_returns.mean()/factor_returns.std()\n",
    "    return sharpe_ratio\n",
    "    \n",
    "\n",
    "\n",
    "project_tests.test_sharpe_ratio(sharpe_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's see what the sharpe ratio for the factors are. Generally, a Sharpe Ratio of near 1.0 or higher is an acceptable single alpha for this universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_annualization_factor = np.sqrt(252)\n",
    "sharpe_ratio(ls_factor_returns, daily_annualization_factor).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: What do you think would happen if we smooth the momentum factor? Would the performance increase, decrease, or no major change? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_#TODO: Put Answer In this Cell_\n",
    "\n",
    "**Answer:**  Performance should increase due to less idiosyncratic noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Combined Alpha Vector\n",
    "\n",
    "To use these alphas in a portfolio, we need to combine them somehow so we get a single score per stock. This is a area where machine learning can be very helpful. In this module, however, we will take the simplest approach of combination: simply averaging the scores from each alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_factors = all_factors.columns[[1, 2, 4]]\n",
    "print('Selected Factors: {}'.format(', '.join(selected_factors)))\n",
    "\n",
    "all_factors['alpha_vector'] = all_factors[selected_factors].mean(axis=1)\n",
    "alphas = all_factors[['alpha_vector']]\n",
    "alpha_vector = alphas.loc[all_factors.index.get_level_values(0)[-1]]\n",
    "alpha_vector.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Portfolio Constrained by Risk Model\n",
    "You have an alpha model and a risk model. Let's find a portfolio that trades as close as possible to the alpha model but limiting risk as measured by the risk model. You'll be building thie optimizer for this portfolio. To help you out. we have provided you with an abstract class called `AbstractOptimalHoldings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class AbstractOptimalHoldings(ABC):    \n",
    "    @abstractmethod\n",
    "    def _get_obj(self, weights, alpha_vector):\n",
    "        \"\"\"\n",
    "        Get the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : CVXPY Variable\n",
    "            Portfolio weights\n",
    "        alpha_vector : DataFrame\n",
    "            Alpha vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        objective : CVXPY Objective\n",
    "            Objective function\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _get_constraints(self, weights, factor_betas, risk):\n",
    "        \"\"\"\n",
    "        Get the constraints\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : CVXPY Variable\n",
    "            Portfolio weights\n",
    "        factor_betas : 2 dimensional Ndarray\n",
    "            Factor betas\n",
    "        risk: CVXPY Atom\n",
    "            Predicted variance of the portfolio returns\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        constraints : List of CVXPY Constraint\n",
    "            Constraints\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _get_risk(self, weights, factor_betas, alpha_vector_index, factor_cov_matrix, idiosyncratic_var_vector):\n",
    "        f = factor_betas.loc[alpha_vector_index].values.T * weights\n",
    "        X = factor_cov_matrix\n",
    "        S = np.diag(idiosyncratic_var_vector.loc[alpha_vector_index].values.flatten())\n",
    "\n",
    "        return cvx.quad_form(f, X) + cvx.quad_form(weights, S)\n",
    "    \n",
    "    def find(self, alpha_vector, factor_betas, factor_cov_matrix, idiosyncratic_var_vector):\n",
    "        weights = cvx.Variable(len(alpha_vector))\n",
    "        risk = self._get_risk(weights, factor_betas, alpha_vector.index, factor_cov_matrix, idiosyncratic_var_vector)\n",
    "        \n",
    "        obj = self._get_obj(weights, alpha_vector)\n",
    "        constraints = self._get_constraints(weights, factor_betas.loc[alpha_vector.index].values, risk)\n",
    "        \n",
    "        prob = cvx.Problem(obj, constraints)\n",
    "        prob.solve(max_iters=500)\n",
    "\n",
    "        optimal_weights = np.asarray(weights.value).flatten()\n",
    "        \n",
    "        return pd.DataFrame(data=optimal_weights, index=alpha_vector.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective and Constraints\n",
    "Using this class as a base class, you'll implement the `OptimalHoldings` class. There's two functions that need to be implemented in this class, the `_get_obj` and `_get_constraints` functions.\n",
    "\n",
    "The `_get_obj` function should return an CVXPY objective function that maximizes $ \\alpha^T * x \\\\ $, where $ x $ is the portfolio weights and $ \\alpha $ is the alpha vector.\n",
    "\n",
    "The `_get_constraints` function should return a list of the following constraints:\n",
    "- $ r \\leq risk_{\\text{cap}}^2 \\\\ $\n",
    "- $ B^T * x \\preceq factor_{\\text{max}} \\\\ $\n",
    "- $ B^T * x \\succeq factor_{\\text{min}} \\\\ \n",
    "- $ x^T\\mathbb{1} = 0 \\\\ $\n",
    "- $ \\|x\\|_1 \\leq 1 \\\\ $\n",
    "- $ x \\succeq weights_{\\text{min}} \\\\ $\n",
    "- $ x \\preceq weights_{\\text{max}} $\n",
    "\n",
    "Where $ x $ is the portfolio weights, $ B $ is the factor betas, and $ r $ is the portfolio risk\n",
    "\n",
    "The first constraint is that the predicted risk be less than some maximum limit. The second and third constraints are on the maximum and minimum portfolio factor exposures. The fourth constraint is the \"market neutral constraint: the sum of the weights must be zero. The fifth constraint is the leverage constraint: the sum of the absolute value of the weights must be less than or equal to 1.0. The last are some minimum and maximum limits on individual holdings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalHoldings(AbstractOptimalHoldings):\n",
    "    def _get_obj(self, weights, alpha_vector):\n",
    "        \"\"\"\n",
    "        Get the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : CVXPY Variable\n",
    "            Portfolio weights\n",
    "        alpha_vector : DataFrame\n",
    "            Alpha vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        objective : CVXPY Objective\n",
    "            Objective function\n",
    "        \"\"\"\n",
    "        assert(len(alpha_vector.columns) == 1)\n",
    "\n",
    "        #TODO: Implement function\n",
    "        \n",
    "        VarA=cvx.quad_form(weights, alpha_vector)\n",
    "        objective_function = cvx.Mimimize(VarA)\n",
    "        \n",
    "        return objective_function\n",
    "    \n",
    "    def _get_constraints(self, weights, factor_betas, risk):\n",
    "        \"\"\"\n",
    "        Get the constraints\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : CVXPY Variable\n",
    "            Portfolio weights\n",
    "        factor_betas : 2 dimensional Ndarray\n",
    "            Factor betas\n",
    "        risk: CVXPY Atom\n",
    "            Predicted variance of the portfolio returns\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        constraints : List of CVXPY Constraint\n",
    "            Constraints\n",
    "        \"\"\"\n",
    "        assert(len(factor_betas.shape) == 2)\n",
    "        \n",
    "        #TODO: Implement function\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def __init__(self, risk_cap=0.05, factor_max=10.0, factor_min=-10.0, weights_max=0.55, weights_min=-0.55):\n",
    "        self.risk_cap=risk_cap\n",
    "        self.factor_max=factor_max\n",
    "        self.factor_min=factor_min\n",
    "        self.weights_max=weights_max\n",
    "        self.weights_min=weights_min\n",
    "\n",
    "\n",
    "project_tests.test_optimal_holdings_get_obj(OptimalHoldings)\n",
    "project_tests.test_optimal_holdings_get_constraints(OptimalHoldings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "With the `OptimalHoldings` class implemented, let's see the weights it generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights = OptimalHoldings().find(alpha_vector, risk_model['factor_betas'], risk_model['factor_cov_matrix'], risk_model['idiosyncratic_var_vector'])\n",
    "\n",
    "optimal_weights.plot.bar(legend=None, title='Portfolio % Holdings by Stock')\n",
    "x_axis = plt.axes().get_xaxis()\n",
    "x_axis.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes. It put most of the weight in a few stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_helper.get_factor_exposures(risk_model['factor_betas'], optimal_weights).plot.bar(\n",
    "    title='Portfolio Net Factor Exposures',\n",
    "    legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize with a Regularization Parameter\n",
    "In order to enforce diversification, we'll use regularization in the objective function. We'll create a new class called `OptimalHoldingsRegualization` which gets its constraints from the `OptimalHoldings` class. In this new class, implement the `_get_obj` function to return a CVXPY objective function that maximize $ \\alpha^T * x + \\lambda\\|x\\|_2\\\\ $, where $ x $ is the portfolio weights, $ \\alpha $ is the alpha vector, and $ \\lambda $ is the regularization parameter.\n",
    "\n",
    "**Note:** * $ \\lambda $ is located in `self.lambda_reg`. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalHoldingsRegualization(OptimalHoldings):\n",
    "    def _get_obj(self, weights, alpha_vector):\n",
    "        \"\"\"\n",
    "        Get the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : CVXPY Variable\n",
    "            Portfolio weights\n",
    "        alpha_vector : DataFrame\n",
    "            Alpha vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        objective : CVXPY Objective\n",
    "            Objective function\n",
    "        \"\"\"\n",
    "        assert(len(alpha_vector.columns) == 1)\n",
    "        \n",
    "        #TODO: Implement function\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def __init__(self, lambda_reg=0.5, risk_cap=0.05, factor_max=10.0, factor_min=-10.0, weights_max=0.55, weights_min=-0.55):\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.risk_cap=risk_cap\n",
    "        self.factor_max=factor_max\n",
    "        self.factor_min=factor_min\n",
    "        self.weights_max=weights_max\n",
    "        self.weights_min=weights_min\n",
    "        \n",
    "\n",
    "project_tests.test_optimal_holdings_regualization_get_obj(OptimalHoldingsRegualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights_1 = OptimalHoldingsRegualization(lambda_reg=5.0).find(alpha_vector, risk_model['factor_betas'], risk_model['factor_cov_matrix'], risk_model['idiosyncratic_var_vector'])\n",
    "\n",
    "optimal_weights_1.plot.bar(legend=None, title='Portfolio % Holdings by Stock')\n",
    "x_axis = plt.axes().get_xaxis()\n",
    "x_axis.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. Well diversified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_helper.get_factor_exposures(risk_model['factor_betas'], optimal_weights_1).plot.bar(\n",
    "    title='Portfolio Net Factor Exposures',\n",
    "    legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize with a Strict Factor Constraints and Target Weighting\n",
    "Another common formulation is to take a predefined target weighting, $x^*$ (e.g., a quantile portfolio), and solve to get as close to that portfolio while respecting portfolio-level constraints. For this next class, `OptimalHoldingsStrictFactor`, you'll implement the `_get_obj` function to minimize on on $ \\|x - x^*\\|_2 $, where $ x $ is the portfolio weights  $ x^* $ is the target weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalHoldingsStrictFactor(OptimalHoldings):\n",
    "    def _get_obj(self, weights, alpha_vector):\n",
    "        \"\"\"\n",
    "        Get the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : CVXPY Variable\n",
    "            Portfolio weights\n",
    "        alpha_vector : DataFrame\n",
    "            Alpha vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        objective : CVXPY Objective\n",
    "            Objective function\n",
    "        \"\"\"\n",
    "        assert(len(alpha_vector.columns) == 1)\n",
    "        \n",
    "        #TODO: Implement function\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "project_tests.test_optimal_holdings_strict_factor_get_obj(OptimalHoldingsStrictFactor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights_2 = OptimalHoldingsStrictFactor(\n",
    "    weights_max=0.02,\n",
    "    weights_min=-0.02,\n",
    "    risk_cap=0.0015,\n",
    "    factor_max=0.015,\n",
    "    factor_min=-0.015).find(alpha_vector, risk_model['factor_betas'], risk_model['factor_cov_matrix'], risk_model['idiosyncratic_var_vector'])\n",
    "\n",
    "optimal_weights_2.plot.bar(legend=None, title='Portfolio % Holdings by Stock')\n",
    "x_axis = plt.axes().get_xaxis()\n",
    "x_axis.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_helper.get_factor_exposures(risk_model['factor_betas'], optimal_weights_2).plot.bar(\n",
    "    title='Portfolio Net Factor Exposures',\n",
    "    legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Now that you're done with the project, it's time to submit it. Click the submit button in the bottom right. One of our reviewers will give you feedback on your project with a pass or not passed grade. You can continue to the next section while you wait for feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
